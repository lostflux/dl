\documentclass[11pt, reqno]{amsart}

\input{~/latex-common/macros.tex}
\usepackage[backend=bibtex,style=science]{biblatex}
% \bibliography{main.bib}
\pgfplotsset{compat=1.18}

\pagestyle{fancy}                         % fancy (allow headers, footers)
\fancyhf{}                                % clear all header/footer settings.
\cfoot{\thepage}                          % set page-numbers in footer.
% \lhead{\textit{\textbf{ Amittai, S}}}   % set name in header, left.
% \rhead{\textsc{Math 71: Algebra}}       % set class name in header, right.
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}


\renewcommand{\theenumi}{\alph{enumi}}

\begin{document}

\newdate{due-date}{27}{04}{2023}

\title{CS-89.31: Deep Learning Generalization and Robustness\\ Amittai Siavava \\ \displaydate{due-date}}
\author{Amittai Siavava}
% \date{\today}


\setlength{\headheight}{13.0pt}
\setlength{\footskip}{15.0pt}

\maketitle

\bigskip

\def \cram { \textsc{cram} }
\def \dom { \textsc{domineering} }
\def \sub { \textsc{subtraction} }
\def \weighted { \textsc{weighted odds and evens}}
\def \nim { \textsc{nim} }
\def \P { \mathbf{P}}
\def \N { \mathbf{N}}

% \newpage
% \begin{problem}[1]
%   Report on the results of the models.

  As we can see in table~\ref{tab:params}, the first neural network,
  having $1024$ hidden units, learns about $4$ times more parameters
  than the second one which has $256$ hidden units.
  This is expected since the number of hidden units in the first neural
  network is exactly four times as many as the number of hidden units in the second.
  However, table!\ref{tab:results} shows that both models have comparable performance
  on the dataset, with a validation error of $0.505$ for the model with $1024$ hidden units,
  compared to $0.515$ for the model with $256$ hidden units.
  In this regard, it may seem that the hidden unit trade-off might be worth it;
  learning fewer parameters yields almost identical error rates.

  \step
  However, when we look at the computed generalization bounds,
  we notice that the $1024$-layer neural network has much larger VC bound,
  L!$_{max}$ bound, and generalization bound by a factor of about $4$.
  Therefore, the $256$-layer neural network is actually more robust,
  suggesting that the $1024$-layer neural network might have learned
  some spurious patterns in the data and is therefore more prone to overfitting.

  \step
    \begin{table}[h]
      \begin{minipage}{.4\textwidth}
        \centering
        \begin{tabular}{| l | r | r |}
          \bottomrule
          Final Result                & Model 1     & Model 2 \\   
          \midrule
          Training Loss               & $1.945$     & $1.959$ \\
          Training Margin             & $-0.983$    & $-0.987$ \\
          Training Error              & $0.468$     & $0.484$	 \\
          Validation Error            & $0.505$     & $0.515$ \\
          \toprule
        \end{tabular}
        \caption{Results.}~\label{tab:results}
      \end{minipage}%
      \begin{minipage}{.6\textwidth}
        \centering
        \begin{tabular}{| l | r | r |}
          \bottomrule
          Metric / Hyperparameter   & Model 1     & Model 2 \\
          \midrule
          Learning Rate             & \multicolumn{2}{c|}{$0.001$}                                    \\ 
          Momentum                  & \multicolumn{2}{c|}{$0.9$}                                      \\ 
          Batch Size                & \multicolumn{2}{c|}{$64$}                                       \\ 
          Epochs                    & \multicolumn{2}{c|}{$25$}                                       \\ 
          Stop Condition            & \multicolumn{2}{c|}{$0.01$}                                     \\
          Dataset                   & \multicolumn{2}{c|}{CIFAR10}                                    \\
          Channels                  & \multicolumn{2}{c|}{$3$}                                        \\
          Classes                   & \multicolumn{2}{c|}{$10$}                                       \\
          \midrule
          \textbf{Hidden Units}     & $\mathbf{1024}$                     & $\mathbf{256}$                      \\
          \midrule
          Frobenius$_1$             & $19.0$                              & $10.1$                              \\
          Frobenius$_2$             & $4.53$                              & $4.36$                              \\
          Distance$_1$              & $4.02$                              & $4.04$                              \\
          Distance$_2$              & $3.15$                              & $2.88$                              \\
          Spectral$_1$              & $1.5$                               & $1.5$                               \\
          Spectral$_2$              & $1.93$                              & $1.84$                              \\
          Fro$^2$                   & $86.0$                              & $44.0$                              \\
          $L1_{max}^2$ 	            & $1300$                              & $725$                               \\
          Spec Dist 	              & $15.0$                              & $13.6$                              \\
          Dist Spec 	              & $248$                               & $119$                               \\
          Spec Dist sum 	          & $263$                               & $133$                               \\
          Spec L1max 	              & $14.8$                              & $13.4$                              \\
          L1max Spec 	              & $240$                               & $115$                               \\
          Spec L1max sum 	          & $255$                               & $129$                               \\
          Dist Fro 	                & $18.2$                              & $17.6$                              \\
          \textbf{Parameters Learned} & $\mathbf{3.16 \times 10^{6}}$     & $\mathbf{7.89 \times 10^{5}}$                \\
          VC bound 	                & $9.19 \times 10^{9}$                & $2.03 \times 10^{9}$                \\
          L1max bound 	            & $3.93 \times 10^{10}$               & $1.21 \times 10^{9}$                \\
          \midrule
          \textbf{Computed Bound}   & $\mathbf{1.88 \times 10^{10}}$      & $\mathbf{4.88 \times 10^{9}}$       \\
          \toprule
        \end{tabular}
        \caption{Result comparisons for the neural network models.}~\label{tab:params}
      \end{minipage}%
    \end{table}


% \end{problem}
\end{document}
